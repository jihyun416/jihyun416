---
layout: post
title:  AWS - ECS와 CodePipeline을 이용한 CI/CD 구성
author: Jihyun
category: aws
tags:
- aws
- codepipeline
- codecommit
- codebuild
- codedeploy
- ecr
- ecs
- docker
date: 2021-12-09 11:00 +0900
---

## 핵심 개념

![](https://jihyun416.github.io/assets/aws_9_0.png)

- **ECR(Elastic Container Registry)**: 개발자가 Docker 컨테이너 이미지를 손쉽게 저장, 관리 및 배포할 수 있게 해주는 완전관리형 Docker 컨테이너 레지스트리
- **ECS(Elastic Container Service)**: Amazon Elastic Container Service(Amazon ECS)는 클러스터에서 컨테이너를 손쉽게 실행, 중지 및 관리할 수 있게 하는 컨테이너 관리 서비스
  - 클러스터(Clusters): 작업(Task) 혹은 서비스(Service)의 논리적 그룹. EC2 혹은 Fargate로 구성 가능
  - 작업 정의(Task Definition): 애플리케이션을 구성하는 하나 이상의 컨테이너를 설명하는 JSON 형식의 텍스트 파일. Task의 청사진
  - 작업(Task): 클러스터 내 작업 정의를 인스턴스화 하는 것
  - 서비스 : ECS 클러스터에서 지정된 수의 작업 정의 인스턴스를 동시에 실행하고 관리해줌. 로드밸런서를 통해 연결된 작업 간에 트래픽 분산 가능




## ECR 리포지토리 생성

방법1) 콘솔에서 Elastic Container Registery에 접속하여 [Create repository]를 통해 Repository 생성

방법2) AWS CLI 를 통해 생성

```sh
aws ecr create-repository --repository-name [리포지토리이름]
```



## ECR에 이미지 업로드(CodePipeline 이용)

실행 시킬 Docker 이미지를 빌드 후 푸시하기 위해 **CodePipeline의 빌드까지의 단계**를 먼저 생성한다. 

*(로컬에서 푸시해도 되지만 빌드 환경이 리눅스가 아니고 어차피 세팅할 부분이니까 먼저한다!)*

### 1) Dockerfile 작성

```dockerfile
FROM public.ecr.aws/docker/library/node:16-slim

RUN apt-get update
RUN apt install yarn -y

RUN mkdir -p /usr/src/app
WORKDIR /usr/src/app

COPY . .
RUN yarn install
RUN yarn build

EXPOSE 8000

CMD ["yarn", "start:prod"]
```

- node 이미지를 docker hub가 아닌 ECR public에서 가져온다.  (Docker pull 제한 때문)
  - docker hub에서 너무 많이 가져오면 만나는 오류 -> toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit


### 2) buildspec.yml 작성

```yaml
version: 0.2

phases:
  pre_build:
    commands:
      - echo Logging in to Amazon ECR..
      - aws ecr get-login-password --region ap-northeast-2 | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com
  build:
    commands:
      - echo Build started on `date`
      - echo Building the Docker image...
      - COMMIT_HASH=$(echo $CODEBUILD_RESOLVED_SOURCE_VERSION | cut -c 1-7)
      - IMAGE_TAG=${COMMIT_HASH:=latest}
      - echo $CODEBUILD_RESOLVED_SOURCE_VERSION
      - echo $IMAGE_TAG
      - docker build -t $IMAGE_REPO_NAME:$IMAGE_TAG .
      - docker tag $IMAGE_REPO_NAME:$IMAGE_TAG $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG
  post_build:
    commands:
      - echo Build completed on `date`
      - echo Pushing the Docker image...
      - docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG
      - printf '{"ImageURI":"%s"}' $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG  > imageDetail.json
artifacts:
  files:
    - imageDetail.json
    - taskdef-stage.json
    - taskdef-prod.json
    - appspec-prod.yml
```

- pre_build : 빌드 전 단계
  - ECR에 로그인 한다. (CodeBuild IAM 권한에 ECR 로그인 권한 있어야 함)
- build : 빌드 단계
  - 소스 버전(CODEBUILD_RESOLVED_SOURCE_VERSION)을 가져와서 가공한다. (태그에 사용 예정)
  - 도커를 빌드한다.
  - 도커 태그를 붙인다.
- post_build: 빌드 후 단계
  - ECR에 도커 이미지를 푸시한다.
  - 도커 이미지주소 정보를 담은 imageDetail.json을 생성한다. (추후 Deploy (ECS blue/green)시 사용
- Artifacts: 빌드 산출물, 추후 Deploy 시 사용하기 위한 파일을 S3에 업로드한다.
  - imageDetail.json : 이미지주소가 담긴 파일, 형식을 맞춰야 Deploy에서 정상적으로 읽힘
  - taskdef-stage.json : Deploy 단계에서 사용할 작업 정의(Task Definition)의 stage 단계용 파일
  - taskdef-prod.json :  Deploy 단계에서 사용할 작업 정의(Task Definition)의 prod 단계용 파일
  - appspec-prod.yml : Deploy 정의가 담긴 파일

### 3) CodePipeline 생성

#### 1> Source Stage

![](https://jihyun416.github.io/assets/aws_9_6.png)

- 파이프라인 이름을 지정함
- 새로운 IAM 역할이 생성되도록 새 서비스 역할을 선택함

![](https://jihyun416.github.io/assets/aws_9_7.png)

- 소스 공급자를 **AWS CodeCommit**으로 선택
- 배포하려는 소스가 있는 **리포지토리 이름**과 **브랜치 이름**을 선택함 (git flow 전략에 맞게 선택)

![](https://jihyun416.github.io/assets/aws_9_8.png)

- 빌드 공급자를 CodeBuild로 생성한다.
- 기존에 생성한 적합한 CodeBuild 프로젝트가 있다면 선택하고 아니면 [프로젝트 생성]을 통해 CodeBuild에 들어가서 프로젝트를 생성해준다.
  - CodeBuild에서 직접 프로젝트 생성을 하면 CodeBuild 프로젝트에서 소스공급자를 지정해야 한다.
  - *파이프라인의 [프로젝트 생성]을 통해서 들어오면 소스 단계가 생략된다. (소스 공급은 파이프라인의 소스 스테이지에서 받는다고 가정하기 때문) CodeBuild에서 직접 프로젝트를 생성하면 소스공급자를 선택해야 한다.*

![](https://jihyun416.github.io/assets/aws_9_9.png)

- CodePipeline에서 사용할 프로젝트를 생성한다.
- 추후 도커 이미지 빌드 Pipeline에서 재사용 하기 위해 docker-codebuild라는 이름으로 생성(파이프라인에 종속되지 않음)

![](https://jihyun416.github.io/assets/aws_9_10.png)

- Codebuild 환경에 대해 선택한다.
- 권한이 있음 "도커 이미지를 빌드하거나 빌드의 권한을 승격하려면 이 플래그를 활성화합니다." <- **매우 중요!! 도커 빌드할 시 꼭 체크**
  - 선택 해야 도커를 빌드할 수 있는 환경이 됨


![](https://jihyun416.github.io/assets/aws_9_11.png)

- buildspec 파일에 빌드를 정의했으므로 buildspec 파일 사용을 선택한다.
- 이름은 buildspec.yml로 저장했으므로 따로 작성하지 않는다. (이름을 다르게 했으면 작성 필요함)
- CloudWatch 로그를 선택해야 빌드 로그를 확인할 수 있다.

![](https://jihyun416.github.io/assets/aws_9_12.png) 

- 프로젝트 생성 이후 파이프라인으로 돌아와 해당 프로젝트를 연결한다.
- 환경변수에서 이 파이프라인 빌드에 필요한 환경변수를 추가해준다. (buildspec.yml에서 이 변수들 참조함)

![](https://jihyun416.github.io/assets/aws_9_13.png)

- 배포 스테이지를 일단 건너뛴다 (ECS에서 서비스 생성 이후 편집을 통해 추가 예정)

![](https://jihyun416.github.io/assets/aws_9_14.png)

- 생성을 하고 나면 파이프라인이 가동된다. (*근데 실패*)
- 세부 정보를 클릭하여 CodeBuild 로그를 확인해 실패 사유를 파악한다.

![](https://jihyun416.github.io/assets/aws_9_15.png)

- 빌드 로그를 확인해보니 ECR 로그인 단계에서 실패했다 -> Codebuild가 ECR에 대한 권한이 없다.
- ECR과 관련된 IAM policy를 CodeBuild가 사용하고 있는 Role에 부여한다. (ECR 로그인, 푸시 권한 필요)

![](https://jihyun416.github.io/assets/aws_9_16.png)

![](https://jihyun416.github.io/assets/aws_9_17.png)

- 위와 같이 정책 추가

![](https://jihyun416.github.io/assets/aws_9_18.png)

- 정책을 추가하고 나니 정상 빌드 되었다.

이제 생성한 이미지를 바탕으로 ECS에 서비스를 배포해 볼 것이다.



## ECS 구성

### 1) 클러스터 생성

![](https://jihyun416.github.io/assets/aws_9_1.png)

- 템플릿은 EC2 Linux를 선택
  - 상시 사용할 웹서버를 올릴 것이기 때문에 Fargate보다는 EC2가 적절함
  - Linux 서버 선택

![](https://jihyun416.github.io/assets/aws_9_2.png)

- 클러스터명을 지정함
- 런칭할 EC2 인스턴스 타입과 개수를 지정해줌
  - 한 클러스터에 여러 유형의 EC2를 지정할 수는 없다. (*t3도 쓰고 r5도 쓸래 이런거 안됨ㅜㅜ*)
  - Auto Scaling을 통해 EC2 인스턴스의 숫자는 가변적으로 조정 가능하다. (ECS 클러스터 생성하면 Auto Scaling Group도 추가되니 확인)
  - 하나의 Task는 여러 EC2에 걸쳐서 런칭될 수는 없기 때문에 작업 크기를 고려하여 인스턴스 유형을 결정하면 된다.

![](https://jihyun416.github.io/assets/aws_9_3.png)

- EC2를 배치할 VPC와 서브넷을 선택한다.
  - private subnet을 선택함(인터넷에서 들어올 때 ALB를 통해서 들어올 것이기 때문에 인터넷 게이트웨이와 연결되지 않은 프라이빗 서브넷 선택)
- 보안그룹은 새로 생성되게 한다. (기존에 쓰던 보안 그룹 중 적용할 만한 것이 있으면 갖다 써도 됨)

![](https://jihyun416.github.io/assets/aws_9_4.png)

- EC2에 적용할 IAM 역할을 ecsInstanceRole로 선택한다. (없으면 새로 생성됨)
  - AmazonEC2ContainerServiceforEC2Role : ECS, ECR 대한 권한이 있음
  - CloudWatchFullAccess : CloudWatch에 대한 권한이 있음
  - 추가> SecretsManagerReadWrite : 어플리케이션에서 Secrets Manager를 사용할 예정이라 생성된 역할에 정책을 추가함

![](https://jihyun416.github.io/assets/aws_9_5.png)

- ECS가 생성되면 원하는 인스턴스 개수만큼 EC2가 생성된다.
- Auto Scaling 전략을 수정하고 싶으면 생성된 Auto Scaling 그룹에서 전략을 수정할 수 있다.
- 보안그룹에서 규칙(Rules)을 수정할 수 있다.
  - 처음 생성 시 80에 대한 허용만 했기 때문에 인바운드 규칙을 추가로 허용해준다.
  - 도커 컨테이너 생성 시 동적 포트 매핑을 위해 임시포트 대역(32768 - 65535)을 열어준다.
  - SSH 접속을 위해서 22 를 열어준다.



### 2) 작업 정의(Task definition) 생성

작업 정의는 json 형식으로 구성되지만, 처음에 이해가 힘드므로 일단 콘솔을 통해 구성해본다.

![](https://jihyun416.github.io/assets/aws_9_19.png)

- EC2에 배포할 것으로 시작 유형 호환성을 EC2로 선택한다.

  ![](https://jihyun416.github.io/assets/aws_9_20.png)

- 태스크 정의 이름을 정의한다. 해당 이름은 taskdef.json의 family 가 된다. 
- 태스크 역할은 없음을 선택한다. (없을 경우 EC2의 IAM Role 사용하는듯, task에서 사용할 role을 따로 지정해주고 싶으면 여기에 연결한다.)
- 네트워크 모드를 브리지로 선택한다. (awsvpc는 동적 포트 매핑을 지원하지 않음)

```json
{
  "requiresCompatibilities": [
    "EC2"
  ],
  "family": "태스크 정의 이름"
  "networkMode": "bridge",
  ...
}
```

![](https://jihyun416.github.io/assets/aws_9_21.png)

- 작업실행 역할은 ecsTaskExecutionRole을 선택한다. (없으면 새 역할생성을 선택하면 생긴다.)
  - ecsTaskExecutionRole에 AmazonS3ReadOnlyAccess 정책을 추가한다. (컨테이너 실행 시 env파일을 S3에서 가져오게 할 때 필요)

- 작업 메모리/작업CPU를 지정한다. (EC2에서 지정한 만큼 가용CPU와 가용 메모리를 사용하게 된다. 마치 지정한 메모리와 CPU 사양의 컴퓨터에서 task가 돌아가는것 처럼 동작하게 된다.)
  - 1vCPU : 1024
  - memory는 mb 단위로 지정
  - cpu와 memory가 EC2 스펙을 초과할 수 없으며, Task 런칭 시 원하는 사양 만큼 가용 cpu와 메모리가 남아있어야 한다.


```json
{
  ...
  "cpu": "512",
  "memory": "1024",
  "executionRoleArn": "arn:aws:iam::416849462746:role/ecsTaskExecutionRole",
  ...
}
```

![](https://jihyun416.github.io/assets/aws_9_23.png)

![](https://jihyun416.github.io/assets/aws_9_22.png)

- [컨테이너 추가]를 통해 작업 내에 포함될 컨테이너(Docker container)를 추가한다.
  - *컨테이너는 여러개 추가할 수 있지만 오토스케일링을 고려할 때 여러 컨테이너가 섞여 있는 설계는 바람직하지 않다.*
- 이미지는 ECR URI를 복사해서 넣는다.
- 포트 매핑은 호스트를 0으로 하면 동적으로 호스트 포트가 매핑된다. ([동적포트 매핑이 필요한 이유](https://ohgym.tistory.com/69))

![](https://jihyun416.github.io/assets/aws_9_25.png)

![](https://jihyun416.github.io/assets/aws_9_24.png)

- 환경변수를 하나하나 입력하지 않고 S3로부터 파일로 공급받음 (arn 입력)

![](https://jihyun416.github.io/assets/aws_9_26.png)

- task에서 내보내는 stdout이 cloudwatch의 로그에 쌓이도록 로그를 구성함 (주의!! 배포 했는데 cloudwatch에 해당하는 이름의 로그그룹이 없으면 배포 시 오류날 수 있음, *작업정의를 콘솔에서 추가했을때는 자동으로 되었는데 파일로 처음 해서 하면 자동 생성이 안되는듯...*)

```json
{
  ...
  "containerDefinitions": [
    {
      "name": "bio-backend-container",
      "image": "이미지URI",
      "essential": true,
      "portMappings": [
        {
          "hostPort": 0,
          "protocol": "tcp",
          "containerPort": 8000
        }
      ],
      "environmentFiles": [
        {
          "value": "arn:aws:s3:::버킷명/환경변수파일.env",
          "type": "s3"
        }
      ],
      "logConfiguration": {
        "logDriver": "awslogs",
        "options": {
          "awslogs-group": "/ecs/태스크의로그그룹명",
          "awslogs-region": "ap-northeast-2",
          "awslogs-stream-prefix": "ecs"
        }
      }
    }
  ]
}
```

![](https://jihyun416.github.io/assets/aws_9_27.png)

- 위와 같이 컨테이너까지 정의 후 작업정의를 생성한다.

- 생성 후 등록되어있는 작업정의에서 JSON 탭을 가보면 JSON 형태로 작업 정의를 확인할 수 있다. (배포 때 이용할 taskdef.json 작성 시 참고)



### 3) 로드밸런서 (Application Load Balancer) 생성

서비스를 생성하기에 앞서 서비스에서 사용할 로드밸런서를 먼저 생성해준다.

![](https://jihyun416.github.io/assets/aws_9_28.png)

- Application Load Balancer 선택

![](https://jihyun416.github.io/assets/aws_9_29.png)

- LB 이름 정의
- 체계는 퍼블릭에 서비스 할 것이므로 "인터넷 경계" 선택
- 리스너는 80, 443 추가

![](https://jihyun416.github.io/assets/aws_9_30.png)

- VPC를 선택하고 각 가용영역 별로 public subnet을 선택함

![](https://jihyun416.github.io/assets/aws_9_31.png)

- HTTPS에 사용할 인증서를 선택 (ACM에 등록된 것을 사용)

![](https://jihyun416.github.io/assets/aws_9_32.png)

- 보안그룹을 새로 생성함 (기존에 있던 것 중 적절한 것이 있으면 사용해도됨)

![](https://jihyun416.github.io/assets/aws_9_33.png)

- 새 대상 그룹을 생성하여 로드밸런서와 연결함
- 대상 상태검사 경로를 등록함 (현재 올릴 어플리케이션은 루트로 헬스체크를 함)

![](https://jihyun416.github.io/assets/aws_9_34.png)

- 대상등록은 등록하지 않고 넘어감 (추후 ECS 서비스를 통해 대상그룹에 대상이 등록될 것임)

![](https://jihyun416.github.io/assets/aws_9_35.png)

- 검토 후 생성함

![](https://jihyun416.github.io/assets/aws_9_36.png)

- 80과 443이 모두 새로 생성한 대상그룹으로 향함
- 규칙을 편집하여 443을 메인으로 하고 80은 무조건 443으로 리다이렉트 하도록 변경

![](https://jihyun416.github.io/assets/aws_9_37.png)

![](https://jihyun416.github.io/assets/aws_9_38.png)

![](https://jihyun416.github.io/assets/aws_9_39.png)

- 로드밸런서의 인증서가 정상 작동하도록 Route53에 ALB를 등록한다. (인증서를 도메인 기준으로 적용했기 때문!)

이제 이 로드밸런서를 서비스 생성 시 이용할 것이다.



### 4) 서비스 생성

생성한 클러스터에 들어가 서비스>생성 을 클릭한다.

![](https://jihyun416.github.io/assets/aws_9_40.png)

- 시작유형을 EC2를 선택한다.
- 사전에 정의한 작업 정의를 선택한다.
- 클러스터를 선택한다. (이미 클러스터에서 들어와서 선택이 되어 있긴 하다)
- 서비스 이름을 정의한다.
- 서비스 유형을 "REPLICA"로 선택한다.
- 작업 개수는 이중화를 위해 2개로 정의한다 (더 많이 해도 됨)
- 최소 정상 상태 백분율과 최대 백분율은 배포 시에 유지 되어야 하는 비율인데 잘못 설정하면 배포가 안될 수 있다. 안전하게 배포가 되고 작업 개수를 유지하도록 하기 위해 100/200 으로 설정한다.

![](https://jihyun416.github.io/assets/aws_9_41.png)

- 배포 방식은 블루/그린 배포를 선택한다. (블루/그린은 CodeDeploy를 이용함)

  - 배포 유형은 서비스 생성 이후 바꿀 수 없다.
  - 롤링 보다 블루/그린이 주는 이점이 더 커서 블루/그린을 선택했다. (롤백 용이, 배포속도 빠름)
- 배포구성은 비율로 하는 것 아닌 한번에 되도록 AllAtOnce 옵션을 선택하였다.
- CodeDeploy를 위한 서비스 역할로 ecsCodeDeployRole을 선택한다. 기존에 생성한 적이 없으면 생성하여 매칭한다.
- 사용 사례 : CodeDeploy - ECS
  - 정책 : AWSCodeDeployRoleForECS


![](https://jihyun416.github.io/assets/aws_9_46.png)

- 상태 검사 유예 기간을 준다.
- 서비스의 IAM 역할은 ecsServiceRole을 선택한다.
- 로드 밸런싱은 Application Load Balancer를 선택 후 아까 생성한 로드밸런서를 선택한 뒤 [로드 밸런서에 추가]를 누른다.

![](https://jihyun416.github.io/assets/aws_9_47.png)

- 프로덕션 리스터 포트를 443으로 선택한다.
- 테스트 리스터 포트를 아무거나 새로 생성해준다. (예시: 8001 로 함)

![](https://jihyun416.github.io/assets/aws_9_48.png)

- 블루/그린 배포를 위해서는 대상그룹이 2개 필요하다.
  - 로드밸런서 생성하면서 만든 대상그룹을 대상그룹 1에 지정해준다.
  - 다른 그룹을 하나 새로 생성해준다.
  - 상태 확인 경로는 health check 할 수 있는 경로를 준다.

![](https://jihyun416.github.io/assets/aws_9_49.png)

![](https://jihyun416.github.io/assets/aws_9_50.png)

- Task의 Auto Scaling을 지정한다.



검토 단계 후 서비스를 생성한다.

서비스가 생성 완료 되면 서비스에 정의된 대로 작업이 시작된다.

CodeDeploy에 해당 서비스에 배포할 수 있는 어플리케이션이 생성된다.

위와 같은 방법으로 stage환경과 production 환경 두개의 클러스터를 생성한다.



## 생성된 CodeDeploy 어플리케이션 확인

ECS 서비스를 블루/그린 배포(AWS CodeDeploy 기반) 으로 생성하면 CodeDeploy 어플리케이션이 생성된다.

![](https://jihyun416.github.io/assets/aws_9_58.png)

- 컴퓨팅 플랫폼 Amazon ECS로 어플리케이션이 생성되었고 어플리케이션 아래 배포그룹이 생성되었다.

![](https://jihyun416.github.io/assets/aws_9_59.png)

- 배포그룹의 설정을 편집을 통해 보면 로드밸런서와 대상그룹(Target Group)에 대한 설정을 볼 수 있다.
- 트래픽 재 라우팅
  - 바로 배포한 환경으로 라우팅할지, 혹은 특정 시간 이후에 할지 지정
- [배포 구성](https://docs.aws.amazon.com/ko_kr/ko_kr/codedeploy/latest/userguide/deployment-configurations.html)
  - 트래픽 이동에 대한 정의, ECSAllAtOnce는 모든 트래픽을 업데이트된 컨테이너로 한번에 이동함
- 원래 개정 종료
  - 기존 컨테이너들을 얼마 뒤에 종료할 것인지 설정한다.
  - 기존 컨테이너들을 살아있으면 새 버전이 문제가 있을 경우 바로 롤백이 가능하다.
  - 개정 종료 시간까지가 CodeDeploy 시간에 포함되므로 Stage 배포는 개정을 바로 종료하고(Stage는 테스트용이기 때문에 롤백이 중요하지 않음), Production 배포는 서비스 버그가 확인 될수 있을 정도로 이전 개정을 살려둔다. (현재 3시간으로 설정)

[Amazon ECS 컴퓨팅 플랫폼의 배포](https://docs.aws.amazon.com/ko_kr/codedeploy/latest/userguide/deployment-steps-ecs.html)

![](https://docs.aws.amazon.com/ko_kr/codedeploy/latest/userguide/images/codedeploy-ecs-deployment-step-2.png)

Green Target group에 새로운 Task 배포

![](https://docs.aws.amazon.com/ko_kr/codedeploy/latest/userguide/images/codedeploy-ecs-deployment-step-3.png)

테스트 리스너를 Green Target group과 연결 (테스트)

![](https://docs.aws.amazon.com/ko_kr/codedeploy/latest/userguide/images/codedeploy-ecs-deployment-step-4.png)

프로덕션 리스터를 Green Target group과 연결 (트래픽 재 라우팅)

![](https://docs.aws.amazon.com/ko_kr/codedeploy/latest/userguide/images/codedeploy-ecs-deployment-step-6.png)

기존 작업 종료(원래 개정 종료)



## 배포 (CodePipeline 이용)

기존 단계에서 파이프라인의 소스-빌드 단계까지 완성을 했다.

빌드 이후 **1) 스테이지 배포 - 2) 수동승인 - 3) 운영 배포** 3단계를 추가해 볼 것이다.

### appspec-prod.yml

```yaml
version: 0.0
Resources:
  - TargetService:
      Type: AWS::ECS::Service
      Properties:
        TaskDefinition: <TASK_DEFINITION>
        LoadBalancerInfo:
          ContainerName: "컨테이너명"
          ContainerPort: 컨테이너포트
```

- Deploy에 대한 정의를 담은 파일이다. 스테이지 배포와 운영 배포에 사용한다.
- <TASK_DEFINITION> 에는 deploy 작업 정의 시 지정한 이름의 파일이 들어간다. 
  - 스테이지 배포에서는 taskdef-stage.json, 운영 배포에서는 taskdef-prod.json을 사용할 예정

### taskdef-stage.json, taskdef-prod.json

```json
{
  "executionRoleArn": "arn:aws:iam::계정:role/ecsTaskExecutionRole",
  "containerDefinitions": [
    {
      "name": "컨테이너명",
      "image": "<IMAGE_URL>",
      "essential": true,
      "portMappings": [
        {
          "hostPort": 0,
          "protocol": "tcp",
          "containerPort": 컨테이너포트
        }
      ],
      "environmentFiles": [
        {
          "value": "arn:aws:s3:::버킷명/환경파일명",
          "type": "s3"
        }
      ],
      "logConfiguration": {
        "logDriver": "awslogs",
        "options": {
          "awslogs-group": "/ecs/로그그룹명",
          "awslogs-region": "ap-northeast-2",
          "awslogs-stream-prefix": "ecs"
        }
      }
    }
  ],
  "requiresCompatibilities": [
    "EC2"
  ],
  "networkMode": "bridge",
  "cpu": "512",
  "memory": "1024",
  "family": "태스크명"
}
```

- 작업정의(Task Definitions)파일을 프로젝트에 추가하고 build artifact에 포함되도록 한다.
- <IMAGE_URL> 은 자리표시자 텍스트로 imageDetail.json에서 ImageURI의 값이 바인딩된다. (빌드 할 때 이미지 이름을 담아서 파일 생성해서 build artifact에 담았음). Deploy 작업 정의 시 [작업 정의의 자리 표시자 텍스트]에 해당 이름을 적어준다.
- 이미지 부분 외에 자리표시자로 가변적으로 값을 받는게 불가하다. (*못찾은 걸수도 있음*🤣) 따라서 스테이지용과 운영용을 따로 만들어준다. (설정이 다르고 환경파일이 다르기 때문에)
- containerDefinitions.name 은 appspec의 ContainerName과 일치해야 한다.
- ContainerPort는 appspec의 containerPort와 일치해야 한다.

### 1) 스테이지 배포 단계 추가

파이프라인 > 편집 > 스테이지 추가 > 작업 추가

![](https://jihyun416.github.io/assets/aws_9_52.png)

- 작업공급자를 Amazon ECS(Blue/Green)로 선택한다.

- 입력 아티팩트를 BuildArifact를 선택한다. (buildspec.yml에서 artifact로 정의했던 그 파일들을 사용함)

- Stage cluster에서 서비스 생성 시 생성 되었던 CodeDeploy 애플리케이션과 배포 그룹을 연결해준다.

- 작업정의는 taskdef-stage.json을 기입한다.

- AppSpec 파일은 app spec-prod.yml을 기입한다. 

  (운영 배포와 파일 동일, 컨테이너명 등 다르게 설정해야 할 부분이 있다면 다르게 할 수 있지만 현재 다른 부분이 없으므로 동일한 파일 적용함)

- 작업 정의의 자리 표시가 텍스트는 taskdef-stage.json에서 자리표시자로 <IMAGE_URL> 을 사용했기 때문에 이와 일치하게 IMAGE_URL이라고 기입한다.

### 2) 수동 승인 단계 추가

스테이지 단계에서 이상이 없을 경우 해당 이미지를 동일하게 운영 환경에 배포할 것이다. 자동으로 배포되지 않도록 수동 승인 단계를 사이에 추가해준다.

![](https://jihyun416.github.io/assets/aws_9_54.png)

- 검토할 시스템의 URL을 추가한다.
- 설명에 전달할 메시지를 추가한다.
- 승인 요청은 SNS을 이용해 추가 세팅하여 원하는 채널에 알림을 보낼 수 있으며, 심플하게 CodePipeline의 알림을 이용해서 Slack에 보낼 수 있다. (이 방법을 이용할것임!)

### 3) 운영 배포 단계 추가

![](https://jihyun416.github.io/assets/aws_9_53.png)

- 스테이지 배포와 같은 전략을 사용하기 때문에 스테이지 작업 설정과 유사하다.
- Production cluster에서 서비스 생성 시 생성 되었던 CodeDeploy 애플리케이션과 배포 그룹을 연결해준다.
- 작업정의는 taskdef-prod.json을 기입한다.
- AppSpec 파일은 app spec-prod.yml을 기입한다.



## 알림 추가

파이프라인에서도 알림이 설정 가능하지만 파이프라인에서 알림을 설정하면 링크가 파이프라인으로 연결되기 때문에, 각 단계에서 유의미한 정보와 연결될 수 있도록 각 서비스에서 알림을 설정할 것이다.

1. CodeCommit 알림은 **Lambda**와 **CodeCommit 트리거**를 이용하여 커밋 내용이 보이도록 커스텀 하여 **Webhook**을 통해 슬랙으로 전송

2. CodeBuild 알림은 빌드 로그를 바로 확인할 수 있도록 **CodeBuild > 프로젝트 > 알림 > 알림 규칙 생성**에서 Build state 전체 체크하고 대상을 Slack으로 연결

   ![](https://jihyun416.github.io/assets/aws_9_56.png)

   *Build phase는 state를 해놨기 때문에 굳이.... 너무 많이 선택하면 시끄럽다*

3. CodeDeploy 알림은 배포를 바로 확인할 수 있도록 **CodeDeploy > 애플리케이션 > 알림 > 알림 규칙 생성**에서 Deployment 전체 체크하고 대상을 Slack으로 연결

   ![](https://jihyun416.github.io/assets/aws_9_55.png)

4. 수동 승인은 별도의 서비스가 없기 때문에 CodePipeline > 프로젝트 > 알림 > 알림 규칙 생성에서 Manual approval 부분을 체크하고 대상을 Slack으로 연결

   ![](https://jihyun416.github.io/assets/aws_9_57.png)

